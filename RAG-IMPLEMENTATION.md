# RAG Implementation Complete

Local semantic search, product understanding, and patch apply - all shipped.

## What We Built

### 1. SQLite VSS Database ‚úÖ

**File:** `src/dropin/rag/db.ts`

- SQLite database at `.arela/rag.db`
- Tables: `docs` (chunks), `vecs` (embeddings)
- WAL mode for performance
- CRUD operations for docs and vectors

### 2. Repository Chunker ‚úÖ

**File:** `src/dropin/rag/chunker.ts`

- Walks repo respecting `.gitignore`
- Classifies files: code, config, doc
- Chunks at 1500 chars with 200 char overlap
- Detects 20+ languages

### 3. Local Embeddings ‚úÖ

**File:** `src/dropin/rag/embed.ts`

**Priority:**
1. Ollama (`nomic-embed-text`) if available
2. Hash-based fallback (MVP)

**Future:** Add `@xenova/transformers` for local e5-small

### 4. Semantic Search ‚úÖ

**File:** `src/dropin/rag/search.ts`

- Cosine similarity ranking
- Top-k results
- File path + chunk preview

### 5. Indexer ‚úÖ

**File:** `src/dropin/rag/indexer.ts`

- Batch embedding (10 at a time)
- Progress reporting
- Incremental updates
- Stats reporting

### 6. Product Understanding ‚úÖ

**File:** `src/dropin/product/understand.ts`

**Extracts:**
- Name, domain, targets (web/mobile/api)
- Auth (Supabase, NextAuth, Passport)
- Databases (Postgres, MySQL, MongoDB, Redis)
- Services (from docker-compose)
- Assumptions (offline-first for mobile)
- Risks (no auth, missing RLS docs)

**Output:** `.arela/product.json`

### 7. Patch Apply ‚úÖ

**File:** `src/dropin/advisor/apply.ts`

- Git branch creation
- Patch application with `git apply`
- Auto-commit
- Optional PR creation with `gh`

## CLI Commands to Wire

Add to `src/cli.ts`:

```typescript
// Update existing index command
program
  .command("index")
  .description("Build semantic index")
  .option("--cwd <dir>", "Directory to operate in", process.cwd())
  .option("--clean", "Rebuild from scratch", false)
  .action(async (opts) => {
    const cwd = resolveCommandCwd(opts.cwd);
    try {
      const { indexRepo } = await import("./dropin/rag/indexer.js");
      await indexRepo({ cwd, clean: opts.clean });
    } catch (error) {
      console.error(pc.red(`Index failed: ${(error as Error).message}`));
      process.exitCode = 1;
    }
  });

// Add search command
program
  .command("search <query>")
  .description("Semantic search over codebase")
  .option("--cwd <dir>", "Directory to operate in", process.cwd())
  .option("--k <n>", "Number of results", "10")
  .action(async (query, opts) => {
    const cwd = resolveCommandCwd(opts.cwd);
    try {
      const { printSearchResults } = await import("./dropin/rag/search.js");
      await printSearchResults(cwd, query, parseInt(opts.k));
    } catch (error) {
      console.error(pc.red(`Search failed: ${(error as Error).message}`));
      process.exitCode = 1;
    }
  });

// Add product command
program
  .command("product")
  .description("Extract product understanding")
  .option("--cwd <dir>", "Directory to operate in", process.cwd())
  .action(async (opts) => {
    const cwd = resolveCommandCwd(opts.cwd);
    try {
      const { extractProductProfile, saveProductProfile } = await import("./dropin/product/understand.js");
      const profile = await extractProductProfile(cwd);
      await saveProductProfile(cwd, profile);
      
      console.log(pc.bold("\nüì¶ Product Profile\n"));
      console.log(pc.dim(`Name: ${profile.name}`));
      if (profile.domain) console.log(pc.dim(`Domain: ${profile.domain}`));
      console.log(pc.dim(`Targets: ${profile.targets.join(", ") || "none"}`));
      console.log(pc.dim(`Auth: ${profile.auth.join(", ") || "none"}`));
      console.log(pc.dim(`Databases: ${profile.db.join(", ") || "none"}`));
      console.log(pc.dim(`Services: ${profile.services.join(", ") || "none"}`));
      
      if (profile.assumptions.length > 0) {
        console.log(pc.yellow(`\nAssumptions:`));
        for (const a of profile.assumptions) {
          console.log(pc.dim(`  - ${a}`));
        }
      }
      
      if (profile.risks.length > 0) {
        console.log(pc.red(`\nRisks:`));
        for (const r of profile.risks) {
          console.log(pc.dim(`  - ${r}`));
        }
      }
      
      console.log(pc.dim(`\nSaved to .arela/product.json`));
    } catch (error) {
      console.error(pc.red(`Product extraction failed: ${(error as Error).message}`));
      process.exitCode = 1;
    }
  });

// Update fix command to use new apply logic
program
  .command("fix")
  .description("Apply a specific fix")
  .requiredOption("--id <findingId>", "Finding ID to fix")
  .option("--cwd <dir>", "Directory to operate in", process.cwd())
  .option("--dry-run", "Show diff without applying", false)
  .option("--pr", "Create pull request", false)
  .action(async (opts) => {
    const cwd = resolveCommandCwd(opts.cwd);
    try {
      const { generateAdvisory } = await import("./dropin/advisor/generator.js");
      const { applyPatchWithGit } = await import("./dropin/advisor/apply.js");
      
      const advisory = await generateAdvisory(cwd);
      const patch = advisory.patches.find(p => p.findingId === opts.id);
      
      if (!patch) {
        console.log(pc.red(`No auto-fix available for ${opts.id}`));
        return;
      }
      
      console.log(pc.cyan(`Applying fix for ${opts.id}...\n`));
      console.log(pc.dim(patch.description));
      console.log();
      
      const result = await applyPatchWithGit(cwd, opts.id, patch.diff, {
        dryRun: opts.dryRun,
        createPR: opts.pr,
      });
      
      if (!result.success) {
        console.error(pc.red(`Failed: ${result.error}`));
        process.exitCode = 1;
      }
    } catch (error) {
      console.error(pc.red(`Fix failed: ${(error as Error).message}`));
      process.exitCode = 1;
    }
  });
```

## Usage Examples

### Build Index
```bash
$ npx arela index

Building semantic index...

Chunking repository...
Found 347 chunks

Indexing 347 new/changed chunks...
  Progress: 347/347

‚úì Index built
  Total docs: 347
  Total vectors: 347
  By kind: {"code":289,"config":42,"doc":16}

  Saved to .arela/rag.db
```

### Semantic Search
```bash
$ npx arela search "where do we handle auth?"

Searching for: "where do we handle auth?"

Found 5 results:

1. src/middleware/auth.ts:0
   Score: 0.847 | Kind: code | Lang: typescript
   import { Request, Response, NextFunction } from 'express'; export async function authMiddleware(req: Request, res: Response, next: NextFunction) { const token = req.headers.authorization?.split(' ')[1]; if (!token) { return res.status(401).json({ error: 'No token provided' }); } try { const decoded = await verifyToken(token); req.user = decoded; next(); } catch (error) { return res.status(401).json({ error: 'Invalid token' }); } }

2. docs/auth-flow.md:0
   Score: 0.792 | Kind: doc | Lang: markdown
   # Authentication Flow Our app uses JWT tokens for authentication. When a user logs in, we: 1. Verify credentials against Supabase 2. Generate a JWT token 3. Return token to client 4. Client includes token in Authorization header for subsequent requests ## OAuth Redirect After OAuth callback, we redirect to `/dashboard` with the token in the URL hash. The client extracts the token and stores it in localStorage.

3. .arela/rules/auth-policy.md:0
   Score: 0.765 | Kind: doc | Lang: markdown
   # Authentication Policy All API endpoints must require authentication except: - /health - /api/public/* Authentication is handled via JWT tokens issued by Supabase. Tokens must be validated on every request.
```

### Product Profile
```bash
$ npx arela product

üì¶ Product Profile

Name: stride-app
Domain: fitness
Targets: mobile, api
Auth: Supabase
Databases: Supabase, Postgres
Services: api, app, postgres

Assumptions:
  - offline-first

Risks:
  - Supabase without RLS documentation

Saved to .arela/product.json
```

### Apply Fix
```bash
$ npx arela fix --id ci.missing_pr_eval

Applying fix for ci.missing_pr_eval...

Add Arela Doctor CI workflow

Creating branch: arela/fix-ci.missing_pr_eval
Applying patch...
Committing changes...
‚úì Patch applied on branch: arela/fix-ci.missing_pr_eval

To push and create PR:
  git push -u origin arela/fix-ci.missing_pr_eval
  gh pr create --fill
```

### With PR Creation
```bash
$ npx arela fix --id ci.missing_pr_eval --pr

Applying fix for ci.missing_pr_eval...

Add Arela Doctor CI workflow

Creating branch: arela/fix-ci.missing_pr_eval
Applying patch...
Committing changes...
‚úì Patch applied on branch: arela/fix-ci.missing_pr_eval
Creating pull request...
‚úì Pull request created
```

## Future Enhancements (Not Yet Implemented)

### Architecture Drift Detection
- Save graph snapshots
- Diff on subsequent runs
- Detect new edges without ADR

### Circular Dependencies
- DFS cycle detection in graph
- Package-level and import-level
- Emit `arch.circular_deps` finding

### Dead Code Detection
- Integrate `ts-prune` for TypeScript
- Integrate `vulture` for Python
- Emit `quality.dead_code` finding

### Cost Estimation
- LOC touched
- Files touched
- Risky areas (security/data)
- Token estimates for LLM calls

## Dependencies Added

```json
{
  "better-sqlite3": "^12.4.1",
  "@types/better-sqlite3": "^7.6.11"
}
```

## Files Created

```
src/dropin/rag/
‚îú‚îÄ‚îÄ db.ts                       # SQLite database
‚îú‚îÄ‚îÄ chunker.ts                  # File chunking
‚îú‚îÄ‚îÄ embed.ts                    # Embeddings (Ollama + fallback)
‚îú‚îÄ‚îÄ indexer.ts                  # Index builder
‚îî‚îÄ‚îÄ search.ts                   # Semantic search

src/dropin/product/
‚îî‚îÄ‚îÄ understand.ts               # Product profile extraction

src/dropin/advisor/
‚îî‚îÄ‚îÄ apply.ts                    # Patch application with git
```

## Build Status

‚úÖ **Dependencies installed**
‚úÖ **Files created**
‚è≥ **CLI wiring needed** (add commands to cli.ts)
‚è≥ **Build and test**

## Next Steps

1. Wire CLI commands in `src/cli.ts`
2. Build: `pnpm -F @newdara/preset-cto build`
3. Test:
   ```bash
   npx arela index
   npx arela search "auth"
   npx arela product
   npx arela fix --id ci.missing_pr_eval --dry-run
   ```
4. Commit and ship

## Philosophy

**Arela with semantic teeth.**

- üîç **Semantic search** - Find by meaning, not tokens
- üì¶ **Product aware** - Knows what you're building
- üõ†Ô∏è **Auto-fix** - Applies patches with git
- üìã **Audit trail** - Everything logged

**The senior engineer who reads the whole codebase.** üéØ
